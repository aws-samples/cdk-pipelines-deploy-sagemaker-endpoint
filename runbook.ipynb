{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bc049ec",
   "metadata": {},
   "source": [
    "# MLOps with CDK\n",
    "This notebook guides the reader through the steps required to show the capabilities of doing MLOps with <a href='https://docs.aws.amazon.com/cdk/latest/guide/home.html'>CDK</a>. We choose a simple use case of classifying hand-written digits to create a Machine Learning model that is registered on SageMaker registry and manually approved by the user. The approval step kicks-off the automated CDK-powered pipeline to deploy this model in Production. \n",
    "\n",
    "#### Index\n",
    "[Data](#data)  \n",
    "[Training](#training)  \n",
    "[Export Artefacts](#export)  \n",
    "[Build Container](#container)  \n",
    "[Register Model](#register)   \n",
    "[Approve Model](#approve)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b253d7",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23f1815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "\n",
    "# Name of the Model Package Group\n",
    "APP_PREFIX = 'cdk-blog'\n",
    "region = 'eu-west-1'\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1b1ef",
   "metadata": {},
   "source": [
    "## Data <a id='data'></a>\n",
    "\n",
    "For the purpose of this notebook we use the <a href='https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html'>Boston</a>  dataset from SKLearn, consisting of 506 samples of houses where the goal is to predict their prices (Regression) given 13 other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0f5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load the digits dataset\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14201c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the flattened feature array and the target array\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Perform standard train-test split with a 20% test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffcdd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1cb09",
   "metadata": {},
   "source": [
    "## Training <a id='training'></a>\n",
    "\n",
    "We create a simple Random Forest regressor with default hyperparameters, and fit it to the Training set.  \n",
    "The obtained Test set RMSE should average around 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ba400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d4ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Evaluate the model Accuracy on the Training and Test set\n",
    "train_mse = mean_squared_error(y_train, model.predict(X_train))\n",
    "test_mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "\n",
    "print(f'MSE on the Training set: {train_mse:.2f}')\n",
    "print(f'MSE on the Test set: {test_mse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae8fdf9",
   "metadata": {},
   "source": [
    "## Export Artefacts <a id='export'></a>\n",
    "We export the model artefact by saving the Estimator using the joblib library. This file is then compressed to a .tar.gz format as required by SageMaker containers, and uploaded to the default S3 bucket associated to the current SageMaker Session. Finally, we save a simple test case locally to test the Inference process at the end.\n",
    "\n",
    "Precondition  \n",
    "If you are executing this notebook using Sagemaker Notebook instance or Sagemaker Studio instance, please make sure that it has IAM role used with AmazonSageMakerFullAccess policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f600e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "\n",
    "# Define the path to the 'model' folder within the current directory, and create it if not present\n",
    "model_dir = Path(\"cdk_pipelines\", \"local\", \"model\")\n",
    "model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the model in a joblib format using the joblib library\n",
    "model_joblib_directory = model_dir / \"model.joblib\"\n",
    "joblib.dump(model, str(model_joblib_directory))\n",
    "print(\"Model saved to {}\".format(model_joblib_directory))\n",
    "\n",
    "model_output_directory = model_dir / \"model.tar.gz\"\n",
    "with tarfile.open(model_output_directory, \"w:gz\") as tar:\n",
    "    tar.add(model_joblib_directory, arcname=os.path.basename(model_joblib_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f011517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "bucket = f\"{APP_PREFIX}-{account}\"\n",
    "\n",
    "try:\n",
    "    # Get the default S3 bucket associated to the current SageMaker session\n",
    "    response = s3_client.create_bucket(\n",
    "        Bucket=bucket,\n",
    "        CreateBucketConfiguration={\n",
    "            'LocationConstraint': region\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    response = s3_client.put_bucket_encryption(\n",
    "        Bucket=bucket,\n",
    "        ServerSideEncryptionConfiguration={\n",
    "            'Rules': [\n",
    "                {\n",
    "                    'ApplyServerSideEncryptionByDefault': {\n",
    "                        'SSEAlgorithm': 'AES256'\n",
    "                    },\n",
    "                    'BucketKeyEnabled': True\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "    )\n",
    "except:\n",
    "    print(f\"bucket already exists with name - {bucket}\")\n",
    "            \n",
    "\n",
    "# Upload the model artefact to the S3 bucket using the same prefix as the local file\n",
    "s3_client.upload_file(str(model_output_directory), bucket, str(model_output_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df3a532",
   "metadata": {},
   "source": [
    "We store a subset of the data to be used to test the deployed endpoint at a later stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ce8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Dump a simple test case to a .json file, used to showcase Inference\n",
    "test_dir = Path(\"cdk_pipelines\", \"local\", \"test\")\n",
    "test_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "payload = {'features': X_test[0].tolist()}\n",
    "\n",
    "with open('./cdk_pipelines/local/test/payload.json', 'w') as f:\n",
    "    json.dump(payload, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ed4a2a",
   "metadata": {},
   "source": [
    "## Build container <a id='container'></a>\n",
    "\n",
    "- laurens to add explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40010d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize cdk_pipelines/code/container/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a8f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize cdk_pipelines/code/container/build_and_push.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7544ea",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!cd cdk_pipelines/code/container; bash build_and_push.sh cdk-blog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d62c6d1",
   "metadata": {},
   "source": [
    "## Register Model <a id='register'></a>\n",
    "After uploading the artefacts to S3, we leverage <a href='https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry.html'>SageMaker Model Registry</a> to manage model versions and deploy them to production. The steps we perform are:\n",
    "* We create a SageMaker Model Package Group for our current use case\n",
    "* We register the newly trained model to this Group\n",
    "* We approve the new Model \n",
    "\n",
    "Note  \n",
    "The following steps can be performed either using the AWS SDK for Python 3 (boto3) or in SageMaker Studio through the UI. We show an example for both by first creating the Model Group using boto3 and then approving the new model using the SageMaker Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68970b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "# Define the input payload to create a Model Package Group with name APP_PREFIX\n",
    "model_package_group_input_dict = {\n",
    "    \"ModelPackageGroupName\" : APP_PREFIX,\n",
    "    \"ModelPackageGroupDescription\" : f\"Model package group for {APP_PREFIX}\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_model_pacakge_group_response = sm_client.create_model_package_group(**model_package_group_input_dict)\n",
    "    print(\"ModelPackageGroup Arn : {}\".format(create_model_pacakge_group_response[\"ModelPackageGroupArn\"]))\n",
    "except Exception:\n",
    "    print(f\"Model Package Group {APP_PREFIX} already created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ECR image with the Inference code for the model\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "INFERENCE_IMAGE = f'{account}.dkr.ecr.{region}.amazonaws.com/cdk-blog:latest' # check your docker image was pushed to the right region\n",
    "\n",
    "# Create the Inference Specification for the model\n",
    "modelpackage_inference_specification = {\n",
    "    \"InferenceSpecification\": {\n",
    "        \"Containers\": [\n",
    "            {\n",
    "                \"Image\": INFERENCE_IMAGE,\n",
    "            }\n",
    "        ],\n",
    "        \"SupportedContentTypes\": [\"application/x-image\"],\n",
    "        \"SupportedResponseMIMETypes\": [\"application/json\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add to the Specification the url where the model is stored\n",
    "model_url = os.path.join(\"s3://\", bucket, model_output_directory)\n",
    "modelpackage_inference_specification[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"] = model_url\n",
    "\n",
    "# Define the input payload to register a Model Package in the Group\n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageGroupName\": APP_PREFIX,\n",
    "    \"ModelPackageDescription\": f\"Model for {APP_PREFIX} stored at {model_url}\",\n",
    "    \"ModelApprovalStatus\": \"PendingManualApproval\",\n",
    "}\n",
    "create_model_package_input_dict.update(modelpackage_inference_specification)\n",
    "\n",
    "# Invoke the SageMaker client to register the model with the given payload\n",
    "create_mode_package_response = sm_client.create_model_package(**create_model_package_input_dict)\n",
    "\n",
    "# Fetch the ARN of the Model Package\n",
    "model_package_arn = create_mode_package_response[\"ModelPackageArn\"]\n",
    "print(\"ModelPackage Version ARN : {}\".format(model_package_arn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6304b1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the Model Package has been published to the Group\n",
    "sm_client.list_model_packages(ModelPackageGroupName=APP_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da75bdb",
   "metadata": {},
   "source": [
    "## Approve Model <a id='approve'></a>\n",
    "If you followed all the steps correctly, you will have registered a model in the Model Package Group you created. Now, you have to approve the model before triggering the pipeline that deploys it to Production.  \n",
    "\n",
    "To approve the model using SageMaker Studio, follow the simple steps highlighted <a href='https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-approve.html'>here</a>. Otherwise, execute the following cell to approve it using boto3.\n",
    "\n",
    "Once the model is approved and you have deploy the model deployment pipeline, this step will automatically trigger the deployment pipeline and deploy the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input payload to approve a Model Package in the Group\n",
    "model_package_update_input_dict = {\n",
    "    \"ModelPackageArn\" : model_package_arn,\n",
    "    \"ModelApprovalStatus\" : \"Approved\"\n",
    "}\n",
    "\n",
    "# Invoke the SageMaker client to approve the model with the given payload\n",
    "model_package_update_response = sm_client.update_model_package(**model_package_update_input_dict)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "377fb665e091ef1263c3ce3dfed9bd42cdf3d6cb0c4b0d26db99a669bc21f5c4"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
